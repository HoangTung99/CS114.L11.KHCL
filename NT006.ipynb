{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NT006.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HoangTung99/CS114.L11.KHCL/blob/master/NT006.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L3WthJql0BQ8"
      },
      "source": [
        "**Bài toán: Nhận diện cảm xúc qua gương mặt người**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0x6g2Lu6j6Pm"
      },
      "source": [
        "**1. Mô tả bài toán**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E0vl-3-tWpdr"
      },
      "source": [
        "Input: Một bức ảnh kích thước 48x48, đã gray scale, có chứa đúng 1 gương mặt người, gương mặt người phải có cảm xúc là 1 trong 6 loại sau (angry, fear, happy, sad, neutral, surprise)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ru8OupomJDqO"
      },
      "source": [
        "![sad_train_45.jpg](data:image/jpeg;base64,/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAIBAQEBAQIBAQECAgICAgQDAgICAgUEBAMEBgUGBgYFBgYGBwkIBgcJBwYGCAsICQoKCgoKBggLDAsKDAkKCgr/wAALCAAwADABAREA/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/9oACAEBAAA/AP0d8O6tphgFhbXChABtGRzWpPaiZPJQszfWrI0DSbXTnn1eZFjVC0jNjp361+aX/BQ3xL8HfHPxHivfgrabrnTJ5IbzVkAW3nnU7jBHnHntjeflyPlYZ5r5J+K1nc3emR+KNLgDzwtmaFT8kyn7yE+pHf8AHua8f8Ra9bajoqQQofMs1aSzLDmS2JxJA2Ocoen0HtX79+ANFk0O1gi8aWF5pV7LAsq2t+NkgU/xDnDDPBx0PBxXXR+MvDtkMQarC7j7uxwSa+Zv26PjV8QH0efwt4eS5jsJYMzzQk/vM8bTjt+NfEHi7wT+2NpekXcXwt8EWmm39/aCRfEniLS0mS0tXJ+SzWYiPzmxvkk5ZQygD05nxN4I1jQNJifxNGTBq9tum2AtFFcdHRSQPlD5AJAypWvlv4iabdeGNe+z28hyJWe2DryHx8ye+4fqK/aj9u/Uf2k/En/BIfQv2gbaS98PfEf4cabb3fjLbGyzJHBm11Q7V+ZlIV5gF5ZUVgea/PH9lr9qP9oP42XN3r3g7xvqkr2CW8nmaxEkEV3vm8tuA7lAAC43Hc46c9P1V/ZJfR/2k/hQ8HjrRI4tZ06Y299byD7xXkMM9VYcjPuK9b/4Un8OJNKjs9X0KC4EabY3kAJUe2en/wBavBP20f2PPBPiX4U3zeE9N2z28byomcnGOQB+v4V+Kn7S3gzWbDULuwvYhHeWj/MzDAYqfkkB9D0PofrX9EHxBvJvip4W8ffDfXYdPXR9a0KWyuII5yS0l1LJZM6uVwYs/NyMhc96/JT9h39nDWvhD471X4a+NNTzfeFvtYaB0VIrpraV4fMQqNrZCDLclScEAivvj4eeNNT+DHxogg0rQFFlfSNbMVMpWUoZBkgL8x/dnleBuwele3/HT9oH4afDKO6ufEF5p8VnFo63KyTXhCPORGzxZUfNtSVCNpJY7hjgV558Gfidc/Gbwguv6cp/s+/W6urWSaUlfsSytEmAwDby642n+A5JyCK/Nr/gqH8B9P07xZP4p8P2qrE7sJ0VcDDHn8PT/wCtX6bf8Ey/2gfC37U3wH8K/FGXTrCd9Q8H21pqMTWsf7ieHdDcRYwdqll5XpXj/wDwU/8AhdN8M7/QfiR8Jvh5Zz6jpdyzWlrZWqqZLUFpLiHgc7omlYDuyrUHwh/aI+G/x4+GSX+iXVrDeXkSXUeo20aK5lx8jggZLDJ568n3rgfGl38Q/idf2PgH4x3eiXOk6ZfLcW17dQ+XFkZwXH3FyOpA7Cuy8WftweAPA2gN8Kf2efhxP4o8QWFmqXY0toobOyVjktJcEEJHvJI4LOeVU8mvDfilpHibxd4UvdU+KWq297e3Sb762soNsUSc5VNxLcA9TycZOOg9V/4I3fAL4w/sh3eq/Bz4maut1HdXs19p7QReVGhYJ5qBAcAbl3AertX2X+1jocmq/Ddtbs7YSXOmSJdwqP4tnLL/AMCUsv41+bvxa/Y+8Y/Bjwhr37RH7I/iEwFTJqV94Guctp2pRlfMMlvjm1lYddnyF85XJJr588Eftm/CP9pWCzg+LFtq2lyRYxY3d472av6kIQG/4Gor6q+Gmg6Xe6Ba6Z8PtO06HTJZRO8mnRpFbyOBw7BCd5HucZq38Y9a8BfDzww9tqepx3t/c5zZxuDLKSP/AB1fUnj69K//2Q==)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t-23mb2wW07u"
      },
      "source": [
        "Ouput: Tên của 1 trong 6 loại cảm xúc (angry, fear, happy, sad, neutral, surprise)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hh4BUioyI9k8"
      },
      "source": [
        "Ví dụ: sad"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yAUUZxx-lYph"
      },
      "source": [
        "**2. Mô tả bộ dữ liệu**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lU7PwKLOXkMf"
      },
      "source": [
        "**Nguồn**: Google Image\n",
        "\n",
        "Thu nhặt ảnh thể hiện cảm xúc của 1 khuôn mặt người từ nhiều nguồn thông qua Google Image với sự hỗ trợ của công cụ hỗ trợ chuẩn hoá ảnh: https://github.com/HoangTung99/CS114.L11.KHCL/blob/master/Face_Detection.ipynb\n",
        "\n",
        "**Công cụ chuẩn hoá để hỗ trợ cho việc thu nhặt ảnh**\n",
        "- Input: Một bức ảnh có ít nhất 1 gương mặt người\n",
        "- Output: 1 cơ số tấm ảnh, với mỗi ảnh sẽ: Chứa đúng 1 gương mặt người, ảnh được gray scale và resize kích thước (48x48)\n",
        "\n",
        "\n",
        "**Quá trình thu nhặt ảnh**\n",
        "1. Tìm kiếm ảnh trên google image\n",
        "2. Đưa ảnh vào công cụ chuẩn hoá\n",
        "3. Chạy công cụ\n",
        "4. Tiến hành kiểm tra từng tấm ảnh trả ra từ công cụ chuẩn hoá với cả 3 điều kiện sau ( cần kiểm tra lại để tránh sự sai sót của công cụ ):\n",
        "    + Chứa đúng 1 gương mặt người\n",
        "    + Đã gray scale\n",
        "    + Đã resize kích thước 48x48\n",
        "5. Tải những ảnh cho ra từ công cụ chuẩn hoá mà thoả mãn 3 điều kiện trên về máy\n",
        "6. Tải lên folder DataSet_CS114 trên google drive theo đường link: https://drive.google.com/drive/folders/1kTAoBLJXEQaj9JOuLZD8xZbCx3I2clUM?usp=sharing\n",
        "\n",
        "**Cấu trúc folder DataSet_CS114:**\n",
        "- Gồm 3 thư mục chính:\n",
        "  + train\n",
        "  + validation\n",
        "  + test\n",
        "- Mỗi thư mục chính, sẽ bao gồm 6 thư mục con:\n",
        "  + angry\n",
        "  + fear\n",
        "  + happy\n",
        "  + sad\n",
        "  + neutral\n",
        "  + surprise\n",
        "\n",
        "**Tổng số ảnh**: 9216\n",
        "\n",
        "\n",
        "**Phân chia dữ liệu**:\n",
        "- Train (60%): 1024 tấm ảnh cho từng loại cảm xúc\n",
        "- Validation (20%): 256 tấm ảnh cho từng loại cảm xúc\n",
        "- Test (20%): 256 tấm ảnh cho từng loại cảm xúc\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WlGiNe1ImF7i"
      },
      "source": [
        "**3. Mô tả về đặc trưng**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GZzBqmzmmsH9"
      },
      "source": [
        "**4. Mô tả thuật toán máy học:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fwe8cZLjmyzf"
      },
      "source": [
        "Dùng model Sequential\n",
        "Mô hình:\n",
        "- 4 khối Convolution ( mỗi khối gồm)\n",
        "  + 1 convolution2D layer để trích xuất đặc trưng\n",
        "  + 1 batchNormalization layer để chuẩn hoá khối batch\n",
        "  + 1 relu layer để khử tuyến tính\n",
        "  + 1 MaxPooling2D để giữ lại đặc trưng quan trọng => giảm độ phức tạp bài toán\n",
        "  + 1 dropout layer để khử bớt dữ liệu => giảm over-fitting\n",
        "- 1 Flattern layer \n",
        "- 2 khối fully connected\n",
        "- 1 Dense Layer với activation function là softmax\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eYdC_Y5wOxBI"
      },
      "source": [
        "**5. Cài đặt, tinh chỉnh tham số**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Frawti3EOxgH"
      },
      "source": [
        "Tiến hành thay đổi các tham số (learning rate, batch_size, epoch, số lượng ảnh):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dZp04JaabHpV"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5u4e7TQTbH5j"
      },
      "source": [
        "!pip install utils\n",
        "!pip install livelossplot"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rp2Oy_7mbPkq"
      },
      "source": [
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import utils\n",
        "import os\n",
        "%matplotlib inline\n",
        "\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.layers import Dense, Input, Dropout,Flatten, Conv2D\n",
        "from tensorflow.keras.layers import BatchNormalization, Activation, MaxPooling2D\n",
        "from tensorflow.keras.models import Model, Sequential\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
        "from tensorflow.keras.utils import plot_model\n",
        "\n",
        "from IPython.display import SVG, Image\n",
        "from livelossplot import PlotLossesKerasTF\n",
        "import tensorflow as tf\n",
        "print(\"Tensorflow version:\", tf.__version__)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "In7l_5TLbRa8"
      },
      "source": [
        "path_dir_data = \"drive/MyDrive/Data_FaceRecognition/\"\n",
        "path_dir_data_train = \"drive/MyDrive/Data_FaceRecognition/train/\"\n",
        "path_dir_data_validation = \"drive/MyDrive/Data_FaceRecognition/validation/\"\n",
        "path_dir_data_test = \"drive/MyDrive/Data_FaceRecognition/test/\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5noKao_KbTCV"
      },
      "source": [
        "for expression in os.listdir(path_dir_data_train):\n",
        "    print(str(len(os.listdir(path_dir_data_train + expression))) + \" \" + expression + \" images\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9PsYkZUG-WCG"
      },
      "source": [
        "**Hyper params**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g5XdrQ5i-UvE"
      },
      "source": [
        "batch_size = 64\n",
        "numberOfEmotions = 6\n",
        "learning_rate = 0.0005\n",
        "min_learning_rate = 0.00001\n",
        "epochs = 10"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PiAie784bV1V"
      },
      "source": [
        "**Generate Training and Validation Batches**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_JWPC1FsbWyw"
      },
      "source": [
        "img_size = 48\n",
        "\n",
        "datagen_train = ImageDataGenerator(horizontal_flip=True)\n",
        "train_generator = datagen_train.flow_from_directory(path_dir_data_train,\n",
        "                                                    target_size=(img_size,img_size),\n",
        "                                                    color_mode=\"grayscale\",\n",
        "                                                    batch_size=batch_size,\n",
        "                                                    class_mode='categorical',\n",
        "                                                    shuffle=True)\n",
        "\n",
        "datagen_validation = ImageDataGenerator(horizontal_flip=True)\n",
        "validation_generator = datagen_validation.flow_from_directory(path_dir_data_validation,\n",
        "                                                    target_size=(img_size,img_size),\n",
        "                                                    color_mode=\"grayscale\",\n",
        "                                                    batch_size=batch_size,\n",
        "                                                    class_mode='categorical',\n",
        "                                                    shuffle=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FLa_M9Wvbadk"
      },
      "source": [
        "**Create Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cfmy23jDbcUp"
      },
      "source": [
        "## Bo sung source auth\n",
        "\n",
        "# First model\n",
        "model = Sequential()\n",
        "\n",
        "# 1 - Convolution\n",
        "model.add(Conv2D(64,(3,3), padding='same', input_shape=(img_size, img_size,1)))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "# 2nd Convolution layer\n",
        "model.add(Conv2D(128,(5,5), padding='same'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "# 3rd Convolution layer\n",
        "model.add(Conv2D(512,(3,3), padding='same'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "# 4th Convolution layer\n",
        "model.add(Conv2D(512,(3,3), padding='same'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "# Flattening\n",
        "model.add(Flatten())\n",
        "\n",
        "# Fully connected layer 1st layer\n",
        "model.add(Dense(256))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "# Fully connected layer 2nd layer\n",
        "model.add(Dense(512))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "model.add(Dense(numberOfEmotions, activation='softmax'))\n",
        "\n",
        "opt = Adam(lr=learning_rate)\n",
        "model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "model.summary()\n",
        "\n",
        "#--------------------------------------------------------------------------\n",
        "#--------------------------------------------------------------------------\n",
        "\n",
        "# # Second Model\n",
        "# model = Sequential()\n",
        "\n",
        "# model.add(Conv2D(64, kernel_size=(3, 3), activation='relu', input_shape=(img_size, img_size, 1)))\n",
        "# model.add(Conv2D(64,kernel_size= (3, 3), activation='relu'))\n",
        "# model.add(MaxPooling2D(pool_size=(2,2), strides=(2, 2)))\n",
        "# model.add(Dropout(0.5))\n",
        "\n",
        "# #2nd convolution layer\n",
        "# model.add(Conv2D(64, (3, 3), activation='relu'))\n",
        "# model.add(Conv2D(64, (3, 3), activation='relu'))\n",
        "# # model.add(BatchNormalization())\n",
        "# model.add(MaxPooling2D(pool_size=(2,2), strides=(2, 2)))\n",
        "# model.add(Dropout(0.5))\n",
        "\n",
        "# #3rd convolution layer\n",
        "# model.add(Conv2D(128, (3, 3), activation='relu'))\n",
        "# model.add(Conv2D(128, (3, 3), activation='relu'))\n",
        "# # model.add(BatchNormalization())\n",
        "# model.add(MaxPooling2D(pool_size=(2,2), strides=(2, 2)))\n",
        "\n",
        "# model.add(Flatten())\n",
        "\n",
        "# #fully connected neural networks\n",
        "# model.add(Dense(1024, activation='relu'))\n",
        "# model.add(Dropout(0.2))\n",
        "# model.add(Dense(1024, activation='relu'))\n",
        "# model.add(Dropout(0.2))\n",
        "\n",
        "# model.add(Dense(numberOfEmotions, activation='softmax'))\n",
        "\n",
        "\n",
        "# #Compliling the model\n",
        "# model.compile(loss=\"categorical_crossentropy\",\n",
        "#               optimizer=Adam(),\n",
        "#               metrics=['accuracy'])\n",
        "\n",
        "# model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "bIzzAV9cbgKE"
      },
      "source": [
        "epochs = 10\n",
        "steps_per_epoch = train_generator.n//train_generator.batch_size\n",
        "validation_steps = validation_generator.n//validation_generator.batch_size\n",
        "\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.1,\n",
        "                              patience=2, min_lr=min_learning_rate, mode='auto')\n",
        "\n",
        "#--------------------------------------------------------------------------\n",
        "#--------------------------------------------------------------------------\n",
        "\n",
        "# reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.1,\n",
        "#                               patience=2, mode='auto')\n",
        "\n",
        "checkpoint = ModelCheckpoint(\"model_weights.h5\", monitor='val_accuracy',\n",
        "                             save_weights_only=True, mode='max', verbose=1)\n",
        "callbacks = [PlotLossesKerasTF(), checkpoint, reduce_lr]\n",
        "\n",
        "history = model.fit(\n",
        "    x=train_generator,\n",
        "    steps_per_epoch=steps_per_epoch,\n",
        "    epochs=epochs,\n",
        "    validation_data = validation_generator,\n",
        "    validation_steps = validation_steps,\n",
        "    callbacks=callbacks\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2DKEGAZybjDz"
      },
      "source": [
        "**Represent Model as JSON String**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6VH3DLv_bj9V"
      },
      "source": [
        "model_json = model.to_json()\n",
        "with open(\"model.json\", \"w\") as json_file:\n",
        "    json_file.write(model_json)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2TpyYuhRbxFP"
      },
      "source": [
        "**Testing & Rating**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rWw5Lalhb0xW"
      },
      "source": [
        "import os  \n",
        "import numpy as np \n",
        "import cv2  \n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        " \n",
        "from keras.models import model_from_json  \n",
        "from keras.preprocessing import image \n",
        "from google.colab.patches import cv2_imshow"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Nl2DHXZb4eO"
      },
      "source": [
        "#load model  \n",
        "model = model_from_json(open(\"drive/MyDrive/Project/model.json\", \"r\").read())  \n",
        "#load weights  \n",
        "model.load_weights('drive/MyDrive/Project/model_weights.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c-I6J6BDcOtR",
        "outputId": "068f1b38-8f7a-4b6b-ca40-5088c4b2567a"
      },
      "source": [
        "# Download pre-train model for detect faces\n",
        "!curl -o haarcascade_frontalface_default.xml https://raw.githubusercontent.com/opencv/opencv/master/data/haarcascades/haarcascade_frontalface_default.xml"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100  908k  100  908k    0     0  2911k      0 --:--:-- --:--:-- --:--:-- 2911k\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "evylkuaIcPpA",
        "outputId": "aa118d65-f758-42a7-a73a-281eaad1e6b0"
      },
      "source": [
        "# download photo\n",
        "!curl -o test_img https://image.shutterstock.com/image-photo/sad-little-girl-pigtails-portrait-260nw-1269412360.jpg"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "\r  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\r100 14686  100 14686    0     0   131k      0 --:--:-- --:--:-- --:--:--  132k\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fAw-v8e5cT5G",
        "outputId": "88a1b86e-dd8b-4b88-cba2-fabd403b36dc"
      },
      "source": [
        "# # load photo\n",
        "# test_img = cv2.imread(\"test_img\")\n",
        "\n",
        "# # convert photo to gray scale\n",
        "# gray_img = cv2.cvtColor(test_img, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "# # detect face\n",
        "# classifier = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')\n",
        "\n",
        "# # perform face detection\n",
        "# faces_detected = classifier.detectMultiScale(gray_img)\n",
        "\n",
        "# for box in faces_detected:  \n",
        "#   x,  y, h, w = box\n",
        "#   cv2.rectangle(gray_img,(x,y),(x+w,y+h),(255,0,0),thickness=2) \n",
        "#   roi_gray=gray_img[y:y+w,x:x+h]#cropping region of interest i.e. face area from  image  \n",
        "#   roi_gray=cv2.resize(roi_gray,(48,48), interpolation = cv2.INTER_AREA)  \n",
        "#   img_pixels = image.img_to_array(roi_gray)  \n",
        "#   img_pixels = np.expand_dims(img_pixels, axis = 0)  \n",
        "#   img_pixels /= 255  \n",
        "  \n",
        "#   print(type(roi_gray))\n",
        "#   print(type(gray_img))\n",
        "#   print(type(test_img))\n",
        "#   predictions = model.predict(img_pixels)  \n",
        "\n",
        "#   #find max indexed array  \n",
        "#   max_index = np.argmax(predictions[0])  \n",
        "\n",
        "#   emotions = ('angry', 'fear', 'happy', 'neutral', 'sad', 'surprise')  \n",
        "#   predicted_emotion = emotions[max_index]  \n",
        "#   text_size = 1\n",
        "#   if h > 400:\n",
        "#     text_size = 2\n",
        "#   elif h > 800:\n",
        "#     text_size = 3\n",
        "#   cv2.putText(test_img, predicted_emotion, (int(x), int(y)), cv2.FONT_HERSHEY_SIMPLEX, text_size, (0,0,255), 2)\n",
        "#   print(max_index)\n",
        "  \n",
        "\n",
        "# cv2_imshow(test_img)  \n",
        "\n",
        "\n",
        "test_img = cv2.imread(\"sad_train_45.jpg\")\n",
        "\n",
        "gray_img = cv2.cvtColor(test_img, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "# roi_gray = gray_img[y: 48, x: 48]\n",
        "# roi_gray = cv2.resize(roi_gray, (48, 48))\n",
        "\n",
        "img_pixels = image.img_to_array(gray_img)  \n",
        "img_pixels = np.expand_dims(img_pixels, axis = 0)  \n",
        "img_pixels /= 255  \n",
        "\n",
        "predictions = model.predict(img_pixels)\n",
        "\n",
        "#find max indexed array  \n",
        "max_index = np.argmax(predictions[0])  \n",
        "\n",
        "emotions = ('angry', 'fear', 'happy', 'neutral', 'sad', 'surprise')  \n",
        "predicted_emotion = emotions[max_index]  \n",
        "\n",
        "print(predicted_emotion)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "surprise\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gBtRfo39gEDW"
      },
      "source": [
        "# emotions = []\n",
        "# for expression in os.listdir(path_dir_data_train + \"angry\"):\n",
        "#     img_path = str(path_dir_data_train + \"angry\" + expression)\n",
        "#     img_pred = cv2.imread(img_path)\n",
        "#     img_pred = cv2.resize(img_pred,(48,48), interpolation = cv2.INTER_AREA)  \n",
        "#     img_pred_pixels = image.img_to_array(img_pred)  \n",
        "#     img_pred_pixels = np.expand_dims(img_pred_pixels, axis = 0)  \n",
        "#     img_pred_pixels /= 255  \n",
        "\n",
        "#     preds = model.predict(img_pred_pixels)\n",
        "\n",
        "#     #find max indexed array  \n",
        "#     max_index = np.argmax(predictions[0])  \n",
        "\n",
        "#     emotions = ('angry', 'fear', 'happy', 'neutral', 'sad', 'surprise')  \n",
        "#     predicted_emotion = emotions[max_index] \n",
        "#     emotions.append(predicted_emotion)\n",
        "i = cv2.imread(r\"drive/MyDrive/DataSet_CS114/train/angryTraining_25739285.jpg\")\n",
        "cv2_imshow(i)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}